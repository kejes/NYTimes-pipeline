{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extracting from Kafka and putting in a raw RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import json\n",
    "import pykafka\n",
    "from pykafka import KafkaClient\n",
    "client = KafkaClient(hosts =\"127.0.0.1:9092\")\n",
    "topic = client.topics[b'kafka_nyt']\n",
    "consumer = topic.get_simple_consumer(consumer_timeout_ms=1000) \n",
    "message_list = [json.loads(message.value) for message in consumer if message is not None]\n",
    "from pyspark import SparkContext, SparkConf\n",
    "if not 'sc' in globals(): # This 'trick' makes sure the SparkContext sc is initialized exactly once\n",
    "    conf = SparkConf().setMaster('local[*]')\n",
    "    sc = SparkContext(conf=conf)\n",
    "rdd = sc.parallelize(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clean RDD and convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loopfunc(group): # Group type is list w/ 40 articles\n",
    "    \"\"\"For every list of articles, loop over every article and keep only relevant attributes for LDA.\"\"\"\n",
    "    final = []\n",
    "\n",
    "    for element in group: # Select only relevant attributes for LDA\n",
    "        abstract = element['abstract']\n",
    "        first_published_date = element['first_published_date']\n",
    "        org_facet = element['org_facet']\n",
    "        title = element['title']\n",
    "        dic={'abstract': abstract, 'first_published_date': first_published_date, \n",
    "             'org_facet': org_facet, 'title' : title}\n",
    "        final.append(dic)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reduced_rdd = rdd.map(lambda x: loopfunc(x)) # Apply the function to every article of every article group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reduced_rdd.first() # Yields correct results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe from the PipelinedRDD\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def convert_to_row(d: dict) -> Row:\n",
    "    return Row(**OrderedDict(sorted(d.items())))\n",
    "\n",
    "df = reduced_rdd.flatMap(lambda x: x).map(lambda x: convert_to_row(x)).toDF() # First flatten the RDD, then convert everything to row\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def truncate_pbdate(date_as_string): # Remove the time for each published date, since we only need the date\n",
    "    return date_as_string.split(\"T\")[0]\n",
    "\n",
    "udfTruncatePbDate = pyspark.sql.functions.udf(truncate_pbdate, StringType())\n",
    "df = df.withColumn(\"first_published_date\", udfTruncatePbDate(\"first_published_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start preprocessing for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Continue with preprocessing + LDA algorithm\n",
    "'''filter the words on stopwords and on re'''\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "def remove_stop(sent):\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    abstract_stopwords = [i for i in word_tokenize(sent.lower()) if i not in stop]\n",
    "    text_out = [re.sub('[^a-zA-Z0-9]','',abstract) for abstract in abstract_stopwords] # Remove special characters\n",
    "    text_out = [word for word in text_out if len(word)>2]     # Remove stopwords and words under X length\n",
    "    \n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Calling the function that cleans the data of abstract '''\n",
    "udf_cleantext = pyspark.sql.functions.udf(remove_stop , ArrayType(StringType()))\n",
    "cleaned_text = df.withColumn(\"abstract\", udf_cleantext('abstract'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TF-IDF\n",
    "TF: HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors. In text processing, a “set of terms” might be a bag of words. The algorithm combines Term Frequency (TF) counts with the hashing trick for dimensionality reduction.\n",
    "\n",
    "IDF: IDF is an Estimator which fits on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Generate a TF-IDF (Term Frequency Inverse Document Frequency) Matrix'''\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer \n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"abstract\", outputCol=\"rawFeatures\", vocabSize = 5000)\n",
    "cvmodel = cv.fit(cleaned_text) #previous cleaned_text\n",
    "featurizedData = cvmodel.transform(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData) # TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Processed data with features'''\n",
    "rescaledData.select('abstract','features').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Applying LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Use LDA to Cluster the TF-IDF Matrix'''\n",
    "\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "lda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\") # 25 topics will result\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Converting topics to textual JSON to visualize in Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = pyspark.sql.functions.udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
    "ldatopics_mapped = ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc)#.show(25,False)\n",
    "\n",
    "# ldaResults = ldamodel.transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''export as json'''\n",
    "topics_json_list = [json.loads(topic) for topic in ldatopics_mapped.toJSON().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for topic in topics_json_list:\n",
    "    item = [{\"topic\": topic['topic'], \n",
    "      \"term\": term, \n",
    "      \"id\":str(example['topic'])+term} for term in topic['topic_desc']]\n",
    "    output.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_flat = [x for y in output for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for item in output_flat:\n",
    "    es.index(index=\"lda\", doc_type='term', id=item['id'], body=item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
